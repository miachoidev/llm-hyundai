import streamlit as st
import pandas as pd
import os
import tempfile
import re
from difflib import SequenceMatcher
from langchain_core.documents import Document as LangchainDocument
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from chunker import convert_docx_to_chunks
from langchain_community.vectorstores.utils import filter_complex_metadata
import concurrent.futures

# Disable Chroma telemetry to avoid protobuf issues
import chromadb
# chromadb.Client = lambda **kwargs: chromadb.Client(telemetry_enabled=False, **kwargs)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì—´ì°¨ ì‚¬ì–‘ì„œ ë¶„ì„ê¸°", page_icon="ğŸš„", layout="wide")

# ì œëª© ë° ì„¤ëª…
st.header("ğŸš„ í˜„ëŒ€ë¡œí…œ - ì—´ì°¨ ê³µê³ ì‚¬ì–‘ ìë™ ë¶„ì„(PoC)")
st.markdown(
    """ğŸ“„ ë³¸ ì‹œìŠ¤í…œì€ PoC(Proof of Concept)ìš©ìœ¼ë¡œ, ê³µê³  ì‚¬ì–‘ì„œ ê¸°ë°˜ ì‚¬ì–‘ ì¶”ì¶œ ë° í‰ê°€ ìë™í™”ë¥¼ ì‹œì—°í•©ë‹ˆë‹¤.  
ì‚¬ì „ì— ì •ë‹µì´ í¬í•¨ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ LLM ì¶”ë¡  ë° ê²€ìƒ‰ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
ì—´ì°¨ ì œì‘ ê³µê³  ì‚¬ì–‘ì„œë¥¼ ì—…ë¡œë“œ í›„ ëª¨ë¸ì„ ì„ íƒí•˜ê³  'ì‚¬ì–‘ ì¶”ì¶œ ì‹œì‘' ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.  

ğŸ¤– ì‚¬ìš©ëª¨ë¸: gpt-4o-mini

ğŸ“Š í‰ê°€:  
LLM ì¶”ë¡  ì„±ëŠ¥ í‰ê°€ : ì •ë‹µ ê¸°ì¤€ F1 / EM(Exact Match)  
RAG ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì • : Recall@K ë°©ì‹ìœ¼ë¡œ ì¸¡ì •  
"""
)

# ì‹œìŠ¤í…œ íë¦„ë„ ì¶”ê°€
with st.expander("ì‹œìŠ¤í…œ í”„ë¡œì„¸ìŠ¤ íë¦„ë„ ë³´ê¸°"):
    st.graphviz_chart("""
    digraph {
        node [shape=box, style=filled, color=lightblue, fontname="ë‚˜ëˆ”ê³ ë”•"];
        
        upload [label="ê³µê³  ì‚¬ì–‘ì„œ ì—…ë¡œë“œ"];
        chunk [label="ì‚¬ì–‘ë¬¸ì„œ ì²­í‚¹ ë° ì„ë² ë”©"];
        query [label="ê³µê³ í•­ëª©ë³„ ê²€ìƒ‰ì¿¼ë¦¬ í™•ì¥"];
        ensemble_search [label="í™•ì¥ì¿¼ë¦¬ë¡œ ensemble_search"];
        llm [label="ê²€ìƒ‰í•œ ì²­í¬ ê¸°ë°˜ LLM ë‹µë³€ ì¶”ì¶œ ìš”ì²­"];
        eval [label="ì„±ëŠ¥ í‰ê°€ (ì •í™•ë„/ê²€ìƒ‰ ì¬í˜„ìœ¨)"];
        
        up;
        chunk -> query;
        query -> ensemble_search;
        ensemble_search -> llm;
        llm -> eval;
        
        {rank=same; upload chunk query ensemble_search}
    }
    """)

# model
model_option = "gpt-4o-mini"

# ë°ì´í„°í”„ë ˆì„ì—ì„œ í‘œì‹œí•  ì—´ ëª©ë¡ ì •ì˜
DISPLAY_COLUMNS = [
    "ë ˆë²¨1",
    "ë ˆë²¨2",
    "ë ˆë²¨3",
    "ë ˆë²¨4",
    "ì •ë‹µ",
    "ì •ë‹µ ëª©ì°¨",
    "LLMì‘ë‹µ",
    "ì°¸ì¡°ë¬¸ì„œëª©ì°¨",
    "ì°¸ì¡°ë¬¸ì„œ",
    "ì •ë‹µì—¬ë¶€",
    "ê²€ìƒ‰ì„±ê³µì—¬ë¶€",
]


# CSV íŒŒì¼ì—ì„œ í…Œì´ë¸” ë°ì´í„° ë¡œë“œ
# @st.cache_data - ìºì‹œ ì œê±° (ê°œë°œ ì¤‘ ë³€ê²½ì‚¬í•­ ì¦‰ì‹œ í™•ì¸ ìœ„í•¨)
def load_table_data():
    try:
        # data í´ë”ì—ì„œ CSV íŒŒì¼ ë¡œë“œ
        csv_path = os.path.join("data", "hd_table 3.csv")
        df = pd.read_csv(csv_path, encoding="utf-8")
        return df
    except Exception as e:
        st.error(f"CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
        return pd.DataFrame(
            {
                "ë ˆë²¨1": ["ì˜¤ë¥˜", "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                "ë ˆë²¨2": ["", ""],
                "ë ˆë²¨3": ["", ""],
                "ë ˆë²¨4": ["", ""],
                "í‘œì¤€ë‹¨ìœ„": ["", ""],
                "ì •ë‹µ": ["", ""],  # ì •ë‹µ ì»¬ëŸ¼ ì¶”ê°€
            }
        )


def generate_queries(row):
    """ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ì¿¼ë¦¬ ìƒì„±"""
    queries = []

    # 1) ë ˆë²¨2,3,4,í‘œì¤€ë‹¨ìœ„ ì¡°í•©
    # nanì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì œì™¸í•˜ê³  ë¬¸ìì—´ ìƒì„±
    level2 = (
        row.get("ë ˆë²¨2", "")
        if row.get("ë ˆë²¨2", "") and pd.notna(row.get("ë ˆë²¨2", ""))
        else ""
    )
    level3 = (
        row.get("ë ˆë²¨3", "")
        if row.get("ë ˆë²¨3", "") and pd.notna(row.get("ë ˆë²¨3", ""))
        else ""
    )
    level4 = (
        row.get("ë ˆë²¨4", "")
        if row.get("ë ˆë²¨4", "") and pd.notna(row.get("ë ˆë²¨4", ""))
        else ""
    )
    std_unit = (
        row.get("í‘œì¤€ ë‹¨ìœ„", "")
        if row.get("í‘œì¤€ ë‹¨ìœ„", "") and pd.notna(row.get("í‘œì¤€ ë‹¨ìœ„", ""))
        else ""
    )

    q1 = f"{level2} {level3} {level4} {std_unit}".strip()
    if q1:
        queries.append({"type": "simple_2_3_4", "query": q1})

    # 2) ë ˆë²¨3,4,í‘œì¤€ë‹¨ìœ„ ì¡°í•©
    q2 = f"{level3} {level4} {std_unit}".strip()
    if q2 and q2 != q1:
        queries.append({"type": "simple_3_4", "query": q2})

    # 3) LLMì„ í†µí•œ ì¿¼ë¦¬ ìƒì„±
    system_prompt = """ë‹¹ì‹ ì€ ì—´ì°¨ ì œì‘ì„ ìœ„í•œ ì‚¬ì–‘ì„œ ë¬¸ì„œì—ì„œ ë¶€í’ˆ ë˜ëŠ” ì„±ëŠ¥ì— ëŒ€í•œ ìš”êµ¬ì‚¬í•­ì„ ì°¾ê¸° ìœ„í•œ ê²€ìƒ‰ ì§ˆì˜ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ **ê³„ì¸µì  ëª©ì°¨ êµ¬ì¡°**ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ê° í•­ëª©ì€ ì‚¬ì–‘ì„œ ì•ˆì—ì„œ ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ìœ¼ë¡œ ê¸°ìˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ì˜ˆì‹œ:
ë¬¸ì„œ ëª©ì°¨ êµ¬ì¡° : 4. ê¸°ìˆ ì‚¬í•­ > 4.3 ì°¨ëŸ‰íŠ¹ì„± > 4.3.8 ì œì–´ê³µê¸° ì••ë ¥  
ë‚´ìš©: "ì£¼ê³µê¸°(MR) 883kPa(9kgf/cmÂ²), ì œë™ì••ë ¥(BC) 490kPa ì´í•˜, ê°ì¢… ê³µì••ì œì–´ì¥ì¹˜ 490kPa(ë™ì‘ë²”ìœ„ 392ï½588kPa)"

ì‚¬ìš©ìê°€ ì°¾ëŠ” í•­ëª©ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì œê³µë©ë‹ˆë‹¤:
ì˜ˆì‹œ ì…ë ¥:  
- ë ˆë²¨1: ìš´ì˜ ì¡°ê±´  
- ë ˆë²¨2: ìš´ì˜ í™˜ê²½  
- ë ˆë²¨3: ìµœì € ì˜¨ë„  
- í‘œì¤€ë‹¨ìœ„: â„ƒ  

ë‹¹ì‹ ì˜ ì—­í• ì€ ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” **5ê°œì˜ ê²€ìƒ‰ ì§ˆì˜(Query)**ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

1. ì‚¬ìš©ìê°€ ì°¾ê³ ì í•˜ëŠ” ì •ë³´ê°€ ë¬¸ì„œ ë‚´ í‘œí˜„ ë°©ì‹ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ **ì „ë¬¸ ìš©ì–´, ìœ ì˜ì–´, ë‹¤ì–‘í•œ ê¸°ìˆ ì  í‘œí˜„**ì„ í™œìš©í•´ì£¼ì„¸ìš”.
2. **ë¬¸ì¥í˜• ì¿¼ë¦¬ ë˜ëŠ” í•µì‹¬ì–´ ì¡°í•©í˜• ì¿¼ë¦¬**ê°€ í˜¼í•©ë˜ì–´ë„ ì¢‹ìŠµë‹ˆë‹¤ (ì˜ˆ: "ìµœì € ì˜¨ë„ëŠ”?" / "ìš´ì˜ ì¡°ê±´ ì˜¨ë„ â„ƒ").
3. **ìˆ«ì ë‹¨ìœ„(ì˜ˆ: mm, kPa, â„ƒ ë“±)ë„ ì¤‘ìš”í•œ ê²€ìƒ‰ í‚¤ì›Œë“œ**ê°€ ë  ìˆ˜ ìˆìœ¼ë‹ˆ í¬í•¨í•´ì£¼ì„¸ìš”.
4. ìµœì¢… ëª©ì ì€ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ê°€ì¥ ì˜ ì°¾ì„ ìˆ˜ ìˆëŠ” ì§ˆì˜ì–´ ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì¶œë ¥ ì˜ˆì‹œ:  
- "ì—´ì°¨ê°€ ìš´í–‰ ê°€ëŠ¥í•œ ìµœì € ì˜¨ë„ëŠ”?"  
- "ê¸°í›„ ì¡°ê±´ì—ì„œì˜ ìµœì € ì˜¨ë„ëŠ” ëª‡ â„ƒì¸ê°€ìš”?"  
- "ì—´ì°¨ì˜ ìµœì†Œ ìš´ì „ ê°€ëŠ¥ ì˜¨ë„"  
- "ìš´ì˜í™˜ê²½ ê¸°ì¤€ ìµœì € ê¸°ì˜¨ â„ƒ"  
- "train minimum operating temperature in Celsius"
    """
    level1 = (
        row.get("ë ˆë²¨1", "")
        if row.get("ë ˆë²¨1", "") and pd.notna(row.get("ë ˆë²¨1", ""))
        else ""
    )
    user_prompt = f"""ì´ì œ ì•„ë˜ í•­ëª©ì— ë”°ë¼ 5ê°œì˜ ê²€ìƒ‰ ì§ˆì˜ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
    ë ˆë²¨1: {level1}
    ë ˆë²¨2: {level2}
    ë ˆë²¨3: {level3} 
    ë ˆë²¨4: {level4}
    í‘œì¤€ë‹¨ìœ„: {std_unit}
    ì¿¼ë¦¬ëŠ” ìˆœì„œëŒ€ë¡œ ë‚˜ì—´ë§Œ í•´ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë²ˆí˜¸ëŠ” í•„ìš” ì—†ìŠµë‹ˆë‹¤."""

    llm = ChatOpenAI(temperature=0.2, model=model_option)
    response = llm.predict(system_prompt + "\n\n" + user_prompt)
    # LLM ì‘ë‹µì„ ì¿¼ë¦¬ ëª©ë¡ìœ¼ë¡œ ë³€í™˜
    llm_queries = [q.strip() for q in response.strip().split("\n") if q.strip()]
    for i, q in enumerate(llm_queries):
        queries.append({"type": f"llm_generated_{i + 1}", "query": q})
    return queries


# ë¹ˆ ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_empty_dataframe():
    # CSVì—ì„œ í…Œì´ë¸” êµ¬ì¡° ë¡œë“œ
    table_df = load_table_data()
    result_df = table_df.copy()

    # ê²°ê³¼ ì—´ ì¶”ê°€ (ë¯¸ë¦¬ ì¶”ê°€í•˜ì—¬ ë°ì´í„° íƒ€ì… ë¬¸ì œ ë°©ì§€)
    if "LLMì‘ë‹µ" not in result_df.columns:
        result_df["LLMì‘ë‹µ"] = ""
    if "ì°¸ì¡°ë¬¸ì„œ" not in result_df.columns:
        result_df["ì°¸ì¡°ë¬¸ì„œ"] = None
    if "ì°¸ì¡°ë¬¸ì„œëª©ì°¨" not in result_df.columns:
        result_df["ì°¸ì¡°ë¬¸ì„œëª©ì°¨"] = ""
    if "ì •ë‹µì—¬ë¶€" not in result_df.columns:
        result_df["ì •ë‹µì—¬ë¶€"] = False
    if "ê²€ìƒ‰ì„±ê³µì—¬ë¶€" not in result_df.columns:
        result_df["ê²€ìƒ‰ì„±ê³µì—¬ë¶€"] = False

    return result_df


# ì„¸ì…˜ ìƒíƒœì— ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™” (ì•±ì´ ì²˜ìŒ ì‹¤í–‰ë  ë•Œë§Œ)
if "result_df" not in st.session_state:
    st.session_state.result_df = initialize_empty_dataframe()

# ì„¸ì…˜ ìƒíƒœì— ë²¡í„° DB ê²°ê³¼ ì´ˆê¸°í™”
if "vector_db_result" not in st.session_state:
    st.session_state.vector_db_result = None

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")

    # íŒŒì¼ ì—…ë¡œë“œ ë²„íŠ¼
    uploaded_file = st.file_uploader("ì‚¬ì–‘ì„œ íŒŒì¼ ì—…ë¡œë“œ (DOCX)", type=["docx"])

    # LLM ëª¨ë¸ ì„ íƒ
    # model_option = st.radio(
    #     "LLM ëª¨ë¸ ì„ íƒ", ["gpt-4o-mini", "claude-3-5-haiku-20241022"]
    # )
    # ì¶”ì¶œ ì‹œì‘ ë²„íŠ¼
    start_button = st.button("ì‚¬ì–‘ ì¶”ì¶œ ì‹œì‘", type="primary", use_container_width=True)

    # ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
    reset_button = st.button("ê²°ê³¼ ì´ˆê¸°í™”", use_container_width=True)

    # ì¶”ê°€ ì •ë³´
    st.info(
        "ì´ ì•±ì€ RAG(Retrieval Augmented Generation)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì—ì„œ ì‚¬ì–‘ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
    )

# ë©”ì¸ ì½˜í…ì¸ 
if uploaded_file is not None:
    st.write("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤:", uploaded_file.name)

# ê²°ê³¼ ì´ˆê¸°í™” ì²˜ë¦¬
if reset_button:
    st.session_state.result_df = initialize_empty_dataframe()
    st.session_state.vector_db_result = None
    st.success("ê²°ê³¼ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ë°ì´í„° í”„ë ˆì„ ì˜ì—­ - í•­ìƒ í‘œì‹œ
result_placeholder = st.empty()
# ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì—´ë§Œ í‘œì‹œ (ë°ì´í„°ëŠ” ëª¨ë‘ ìœ ì§€)
result_placeholder.dataframe(
    st.session_state.result_df[DISPLAY_COLUMNS], use_container_width=True
)

# ì„±ëŠ¥ í‰ê°€ ë²„íŠ¼ ì˜ì—­
evaluate_placeholder = st.empty()

# ìƒíƒœ í‘œì‹œ ì˜ì—­
status_placeholder = st.empty()

# ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì˜ì—­
download_placeholder = st.empty()


# ì„±ëŠ¥ í‰ê°€ llm ì²˜ë¦¬
def evaluate_llm(gold_answer, gold_doc, pred, ref_doc):
    llm = ChatOpenAI(temperature=0.2, model=model_option)
    ev_prompt = f"""ë‹¤ìŒì€ ì •ë‹µê³¼ ì •ë‹µì˜ ê·¼ê±° ë¬¸ì„œ ì…ë‹ˆë‹¤.
    ì •ë‹µ: {gold_answer}
    ì •ë‹µ ì¶œì²˜: {gold_doc}
    
    ìœ„ ì •ë‹µê³¼ ìë™ ì¶”ì¶œ ê²°ê³¼ ê°’ì´ ê°™ì€ì§€ true/falseë¡œ ë‹µí•´ì£¼ì„¸ìš”. ìˆ«ì ê°’ì´ë‚˜ ë‹¨ìœ„, ì„¤ëª… ë“± ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ê²½ìš° ê°™ì€ ê°’ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
    ë˜í•œ ragë¡œ ê²€ìƒ‰í•œ ì²­í¬ë“¤ ì¤‘ ìœ„ ì •ë‹µ ì¶œì²˜ ë¬¸ì„œ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ true/falseë¡œ ë‹µí•´ì£¼ì„¸ìš”. 
    ì¶”ì¶œ ê²°ê³¼: {pred}
    rag ê²€ìƒ‰í•œ ì°¸ì¡° ì²­í¬ë“¤: {ref_doc}

    ë‹µë³€ í˜•ì‹:
    ì •ë‹µ ì¼ì¹˜ ì—¬ë¶€: true/false
    ì²­í¬ ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€: true/false
    """
    response = llm.predict(ev_prompt)

    answer_correct = (
        response.split("ì •ë‹µ ì¼ì¹˜ ì—¬ë¶€:")[1].split("ì²­í¬ ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€:")[0].strip()
        == "true"
    )
    search_success = response.split("ì²­í¬ ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€:")[1].strip() == "true"

    return answer_correct, search_success


# ì„±ëŠ¥ í‰ê°€ ì²˜ë¦¬
def evaluate_performance():
    status_placeholder.info("ì„±ëŠ¥ í‰ê°€ ì§„í–‰ ì¤‘...")

    # ë³‘ë ¬ ì²˜ë¦¬ìš© í•¨ìˆ˜ ì •ì˜
    def process_evaluation(args):
        index, row = args
        try:
            answer_correct, search_success = evaluate_llm(
                row["ì •ë‹µ"], row["ì •ë‹µ ë¬¸ì„œ"], row["LLMì‘ë‹µ"], row["ì°¸ì¡°ë¬¸ì„œ"]
            )
            return index, answer_correct, search_success
        except Exception as e:
            print(f"í•­ëª© {index} í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return index, False, False

    # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = [
            executor.submit(process_evaluation, (index, row))
            for index, row in st.session_state.result_df.iterrows()
        ]

        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        results_dict = {"ì •ë‹µì—¬ë¶€": {}, "ê²€ìƒ‰ì„±ê³µì—¬ë¶€": {}}

        # ê²°ê³¼ê°€ ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ì²˜ë¦¬
        for future in concurrent.futures.as_completed(futures):
            try:
                index, answer_correct, search_success = future.result()
                results_dict["ì •ë‹µì—¬ë¶€"][index] = answer_correct
                results_dict["ê²€ìƒ‰ì„±ê³µì—¬ë¶€"][index] = search_success
            except Exception as e:
                print(f"í‰ê°€ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¼ê´„ í• ë‹¹
    for index, is_correct in results_dict["ì •ë‹µì—¬ë¶€"].items():
        st.session_state.result_df.loc[index, "ì •ë‹µì—¬ë¶€"] = (
            "true" if is_correct else "false"
        )

    for index, is_success in results_dict["ê²€ìƒ‰ì„±ê³µì—¬ë¶€"].items():
        st.session_state.result_df.loc[index, "ê²€ìƒ‰ì„±ê³µì—¬ë¶€"] = (
            "true" if is_success else "false"
        )

    # ìŠ¤íƒ€ì¼ë§ í•¨ìˆ˜ ì •ì˜
    def style_dataframe(df):
        # ì •ë‹µì—¬ë¶€ì™€ ê²€ìƒ‰ì„±ê³µì—¬ë¶€ì— ìŠ¤íƒ€ì¼ ì ìš©
        styler = df.style.applymap(
            lambda v: (
                "background-color: #CCFFCC"
                if v == "true"
                else "background-color: #FFCCCC"
            ),
            subset=["ì •ë‹µì—¬ë¶€", "ê²€ìƒ‰ì„±ê³µì—¬ë¶€"],
        )
        return styler

    # ìŠ¤íƒ€ì¼ ì ìš©í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
    styled_df = style_dataframe(st.session_state.result_df[DISPLAY_COLUMNS])
    result_placeholder.dataframe(styled_df, use_container_width=True)

    # ì •ë‹µë¥  ê³„ì‚° (ì›ë˜ ë¶ˆë¦¬ì–¸ ê°’ìœ¼ë¡œ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ ë¬¸ìì—´ì„ ë‹¤ì‹œ ë¶ˆë¦¬ì–¸ìœ¼ë¡œ ë³€í™˜)
    correct = (st.session_state.result_df["ì •ë‹µì—¬ë¶€"] == "true").sum()
    total = len(st.session_state.result_df)
    accuracy = correct / total if total > 0 else 0

    # ê²€ìƒ‰ ì¬í˜„ìœ¨ ê³„ì‚°
    hit = (st.session_state.result_df["ê²€ìƒ‰ì„±ê³µì—¬ë¶€"] == "true").sum()
    recall = hit / total if total > 0 else 0

    st.session_state.evaluation_result = (
        f"ì •ë‹µë¥ : {accuracy:.0%} ({correct}/{total}), "
        f"ê²€ìƒ‰ ì¬í˜„ìœ¨: {recall:.0%} ({hit}/{total})"
    )

    status_placeholder.success("ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ!")


# ë¶„ì„ ì²˜ë¦¬
if start_button and uploaded_file is not None:
    # ìƒíƒœ ë©”ì‹œì§€ í‘œì‹œ
    status_placeholder.info("ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ë²¡í„° DB ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    try:
        chunks = convert_docx_to_chunks(uploaded_file)

        # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = OpenAIEmbeddings()

        # Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„± - ê³µì‹ ë¬¸ì„œ ë°©ì‹ëŒ€ë¡œ
        vectorstore = Chroma(
            collection_name="langchain",
            embedding_function=embeddings,
            # persist_directory=persist_directory,
        )
        k = 3
        vectorstore.add_documents(chunks)
        vector_retriever = vectorstore.as_retriever(search_kwargs={"k": k})

        # BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        bm25_retriever = BM25Retriever.from_documents(chunks)

        ensemble = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[0.5, 0.5],
        )

        # ë¬¸ì„œ ì²­í‚¹/ë²¡í„°í™” ì™„ë£Œ í›„ ë°ì´í„° ì¶”ì¶œ ì‹œì‘
        status_placeholder.success("ë¬¸ì„œ ë²¡í„°í™” ì™„ë£Œ. ì‚¬ì–‘ ì •ë³´ ë³‘ë ¬ ì¶”ì¶œ ì¤‘...")

        # ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
        def process_row(args):
            index, row = args
            ensemble_docs = []
            queries = generate_queries(row)
            for query in queries:
                docs = ensemble.get_relevant_documents(query["query"])
                ensemble_docs.extend(docs)

            # ì¤‘ë³µ ì œê±°
            unique_docs = {}
            for doc in ensemble_docs:
                # doc ê°ì²´ì˜ id í•„ë“œë¥¼ ì‚¬ìš©
                if hasattr(doc, "id"):
                    doc_key = doc.id
                else:
                    doc_key = doc.page_content
                unique_docs[doc_key] = doc
            ensemble_docs = list(unique_docs.values())

            # ë¹ˆ ì¿¼ë¦¬ ì œì™¸
            filtered_queries = [
                query["query"] for query in queries if query["query"].strip()
            ]
            query_used = ", ".join(filtered_queries)
            answer_prompt = f"""ë‹¹ì‹ ì€ ì—´ì°¨ ì œì‘ ì‚¬ì–‘ì„œì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì—´ì°¨ ì œì‘ ì‚¬ì–‘ì„œì˜ ì—¬ëŸ¬ ë¶€ë¶„ì…ë‹ˆë‹¤:
{ensemble_docs}
ìœ„ ë¬¸ì„œì—ì„œ ì•„ë˜ ì§ˆì˜ë“¤ì— ëŒ€í•œ ë‹µë³€ê³¼ ì°¸ì¡° ë¬¸ì„œì˜ ëª©ì°¨ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.  
1), ê°€) ë“± ì„¸ë¶€ ëª©ì°¨ êµ¬ë¶„ìëŠ” í¬í•¨í•˜ë˜, **ë¬¸ì„œ ì œëª©ì´ë‚˜ ë‚´ìš©ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.** ì•„ë˜ ì§ˆì˜ë“¤ì€ ëª¨ë‘ ë™ì¼ í•­ëª©ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.  
ë¬¸ì„œì— ê°’ì´ ëª…ì‹œë˜ì–´ ìˆë‹¤ë©´ ë‹¨ìœ„ë„ í•¨ê»˜ í‘œì‹œí•´ì£¼ì„¸ìš”.  

ğŸ’¡ **ë°˜ë“œì‹œ ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ ê°„ê²°í•œ í˜•ì‹**ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:  
ì˜ˆì‹œ1)  
ë‹µë³€: 1000mm, ì°¸ì¡°ë¬¸ì„œ: 3.3 1)  
ì˜ˆì‹œ2)  
ë‹µë³€: ì² ë„ ê·œì •ì— ë”°ë¥¸ë‹¤., ì°¸ì¡°ë¬¸ì„œ: 4.3.8 1)  
ì˜ˆì‹œ3)  
ë‹µë³€: 75dB, ì°¸ì¡°ë¬¸ì„œ: 4.3.11.2 1) ê°€)

â—ì°¸ì¡°ë¬¸ì„œëŠ” **ìˆ«ì, 1), ê°€)** ë“± **ëª©ì°¨ ì •ë³´ë§Œ í¬í•¨í•˜ê³  ì œëª©ì´ë‚˜ ë³¸ë¬¸ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**  
ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì§ˆë¬¸: '{query_used}'"""

            llm = ChatOpenAI(temperature=0.2, model=model_option)
            answer = llm.predict(answer_prompt)
            values = answer.split("ë‹µë³€:")[1].split(", ì°¸ì¡°ë¬¸ì„œ:")[0].strip()
            doc_index = answer.split("ì°¸ì¡°ë¬¸ì„œ:")[-1].strip()

            return index, values, doc_index, str(ensemble_docs)

        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [
                executor.submit(process_row, (index, row))
                for index, row in st.session_state.result_df.iterrows()
            ]

            # ê²°ê³¼ê°€ ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ì²˜ë¦¬
            for future in concurrent.futures.as_completed(futures):
                try:
                    index, values, doc_index, docs_str = future.result()

                    # ê°’ í• ë‹¹ ì‹œ ë°ì´í„° íƒ€ì… ë¬¸ì œ ë°©ì§€
                    st.session_state.result_df.loc[index, "LLMì‘ë‹µ"] = str(values)
                    st.session_state.result_df.loc[index, "ì°¸ì¡°ë¬¸ì„œëª©ì°¨"] = str(
                        doc_index
                    )
                    st.session_state.result_df.loc[index, "ì°¸ì¡°ë¬¸ì„œ"] = docs_str

                    # ê° í•­ëª© ì²˜ë¦¬ í›„ UI ì—…ë°ì´íŠ¸
                    result_placeholder.dataframe(
                        st.session_state.result_df[DISPLAY_COLUMNS],
                        use_container_width=True,
                    )
                except Exception as e:
                    print(f"í•­ëª© {index} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        evaluate_performance()
        if st.session_state.evaluation_result:
            status_placeholder.success(st.session_state.evaluation_result)

        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í™œì„±í™”
        csv = st.session_state.result_df.to_csv(index=False)
        download_placeholder.download_button(
            label="CSVë¡œ ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name="ì—´ì°¨_ì‚¬ì–‘_ì¶”ì¶œê²°ê³¼.csv",
            mime="text/csv",
        )

    except Exception as e:
        status_placeholder.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

elif start_button and uploaded_file is None:
    st.error("ë¨¼ì € ì‚¬ì–‘ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
