import streamlit as st
import pandas as pd
import os
import tempfile
import re
from difflib import SequenceMatcher
from langchain_core.documents import Document as LangchainDocument
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from chunker import convert_docx_to_chunks
from langchain_community.vectorstores.utils import filter_complex_metadata

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì—´ì°¨ ì‚¬ì–‘ì„œ ë¶„ì„ê¸°", page_icon="ğŸš„", layout="wide")

# ì œëª© ë° ì„¤ëª…
st.title("ğŸš„ í˜„ëŒ€ë¡œí…œ - ì—´ì°¨ ê³µê³ ì‚¬ì–‘ ìë™ ë¶„ì„(PoC)")
st.markdown(
    """ğŸ“„ ë³¸ ì‹œìŠ¤í…œì€ PoC(Proof of Concept)ìš©ìœ¼ë¡œ, ê³µê³  ì‚¬ì–‘ì„œ ê¸°ë°˜ ì‚¬ì–‘ ì¶”ì¶œ ë° í‰ê°€ ìë™í™”ë¥¼ ì‹œì—°í•©ë‹ˆë‹¤.  
ğŸ§ª ì‚¬ì „ì— ì •ë‹µì´ í¬í•¨ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ LLM ì¶”ë¡  ë° ê²€ìƒ‰ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.  

ğŸ”§ ì‚¬ìš© ë°©ë²•:  
1ï¸âƒ£ ì—´ì°¨ ì œì‘ ê³µê³  ì‚¬ì–‘ì„œë¥¼ ì—…ë¡œë“œ í›„ ëª¨ë¸ì„ ì„ íƒí•˜ê³  'ì‚¬ì–‘ ì¶”ì¶œ ì‹œì‘' ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.  
2ï¸âƒ£ RAG (Retrieval-Augmented Generation) ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ ë¬¸ì„œì—ì„œ ìŠ¤í™ ì •ë³´ë¥¼ ìë™ ì¶”ì¶œí•©ë‹ˆë‹¤.  
3ï¸âƒ£ ë¶„ì„ ê²°ê³¼ëŠ” CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.  

ğŸ“Š ëª¨ë¸ í‰ê°€: ë³¸ PoCëŠ” ì‚¬ì–‘ì„œ ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ ì •í™•ë„ì™€ ê²€ìƒ‰ ì¬í˜„ìœ¨ì´ í•µì‹¬  
LLM ì¶”ë¡  ì„±ëŠ¥ í‰ê°€ : ì •ë‹µ ê¸°ì¤€ F1 / EM(Exact Match)  
RAG ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì • : Recall@K ë°©ì‹ìœ¼ë¡œ ì¸¡ì •  
"""
)


# ì •ë‹µ í‰ê°€ í•¨ìˆ˜ ì •ì˜
def extract_numbers(text):
    if not isinstance(text, str):
        return []
    return re.findall(r"\d+\.?\d*", text)


def is_answer_correct(pred, answer):
    if not isinstance(pred, str) or not isinstance(answer, str):
        return False

    pred_numbers = extract_numbers(pred)
    answer_numbers = extract_numbers(answer)

    # ìˆ«ì ë¹„êµ ìš°ì„ 
    if pred_numbers and answer_numbers:
        return pred_numbers == answer_numbers

    # ìˆ«ìê°€ ì—†ê±°ë‚˜ ë¹„êµ ë¶ˆê°€ëŠ¥í•˜ë©´ ë¬¸ìì—´ ìœ ì‚¬ë„ ë¹„êµ (ì„œìˆ í˜•)
    similarity = SequenceMatcher(None, pred.strip(), answer.strip()).ratio()
    return similarity >= 0.9  # ì„ê³„ê°’ì€ ìƒí™©ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥


def is_correct_chunk(answer_chunk, reference_chunks):
    # ë¬¸ìì—´ ë¹„êµê°€ ì•„ë‹Œ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì •ë‹µ ë¬¸ì„œ ë‚´ìš© í™•ì¸
    if isinstance(reference_chunks, str) and "Document" in reference_chunks:
        try:
            # ì°¸ì¡° ë¬¸ì„œê°€ ë¬¸ìì—´ë¡œ ì €ì¥ëœ ê²½ìš°, page_content ê°’ë“¤ì„ ì¶”ì¶œ
            for chunk in reference_chunks.split("Document("):
                if answer_chunk in chunk and "page_content" in chunk:
                    # page_content ë¶€ë¶„ì—ì„œ ì •ë‹µ ë¬¸ì„œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    content_part = (
                        chunk.split("page_content='")[1].split("')")[0]
                        if "page_content='" in chunk
                        else ""
                    )
                    if content_part and answer_chunk in content_part:
                        return True
            return False
        except Exception:
            # íŒŒì‹± ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë¹„êµ
            return answer_chunk == reference_chunks
    else:
        # ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (ë¬¸ìì—´ ì§ì ‘ ë¹„êµ)
        return answer_chunk == reference_chunks


# CSV íŒŒì¼ì—ì„œ í…Œì´ë¸” ë°ì´í„° ë¡œë“œ
# @st.cache_data - ìºì‹œ ì œê±° (ê°œë°œ ì¤‘ ë³€ê²½ì‚¬í•­ ì¦‰ì‹œ í™•ì¸ ìœ„í•¨)
def load_table_data():
    try:
        # data í´ë”ì—ì„œ CSV íŒŒì¼ ë¡œë“œ
        csv_path = os.path.join("data", "hd_table 3.csv")
        df = pd.read_csv(csv_path, encoding="utf-8")
        return df
    except Exception as e:
        st.error(f"CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
        return pd.DataFrame(
            {
                "ë ˆì´ë¸”1": ["ì˜¤ë¥˜", "CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"],
                "ë ˆì´ë¸”2": ["", ""],
                "ë ˆì´ë¸”3": ["", ""],
                "ë ˆì´ë¸”4": ["", ""],
                "í‘œì¤€ë‹¨ìœ„": ["", ""],
                "ì •ë‹µ": ["", ""],  # ì •ë‹µ ì»¬ëŸ¼ ì¶”ê°€
            }
        )


def generate_queries(row):
    """ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ì¿¼ë¦¬ ìƒì„±"""
    queries = []

    # 1) ë ˆë²¨2,3,4,í‘œì¤€ë‹¨ìœ„ ì¡°í•©
    # nanì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì œì™¸í•˜ê³  ë¬¸ìì—´ ìƒì„±
    level2 = (
        row.get("ë ˆë²¨2", "")
        if row.get("ë ˆë²¨2", "") and pd.notna(row.get("ë ˆë²¨2", ""))
        else ""
    )
    level3 = (
        row.get("ë ˆë²¨3", "")
        if row.get("ë ˆë²¨3", "") and pd.notna(row.get("ë ˆë²¨3", ""))
        else ""
    )
    level4 = (
        row.get("ë ˆë²¨4", "")
        if row.get("ë ˆë²¨4", "") and pd.notna(row.get("ë ˆë²¨4", ""))
        else ""
    )
    std_unit = (
        row.get("í‘œì¤€ ë‹¨ìœ„", "")
        if row.get("í‘œì¤€ ë‹¨ìœ„", "") and pd.notna(row.get("í‘œì¤€ ë‹¨ìœ„", ""))
        else ""
    )

    q1 = f"{level2} {level3} {level4} {std_unit}".strip()
    if q1:
        queries.append({"type": "simple_2_3_4", "query": q1})

    # 2) ë ˆë²¨3,4,í‘œì¤€ë‹¨ìœ„ ì¡°í•©
    q2 = f"{level3} {level4} {std_unit}".strip()
    if q2 and q2 != q1:
        queries.append({"type": "simple_3_4", "query": q2})

    # 3) LLMì„ í†µí•œ ì¿¼ë¦¬ ìƒì„±
    system_prompt = """ë‹¹ì‹ ì€ ì—´ì°¨ ì œì‘ì„ ìœ„í•œ ì‚¬ì–‘ì„œ ë¬¸ì„œì—ì„œ ë¶€í’ˆ ë˜ëŠ” ì„±ëŠ¥ì— ëŒ€í•œ ìš”êµ¬ì‚¬í•­ì„ ì°¾ê¸° ìœ„í•œ ê²€ìƒ‰ ì§ˆì˜ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ **ê³„ì¸µì  ëª©ì°¨ êµ¬ì¡°**ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ê° í•­ëª©ì€ ì‚¬ì–‘ì„œ ì•ˆì—ì„œ ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ìœ¼ë¡œ ê¸°ìˆ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ì˜ˆì‹œ:
ë¬¸ì„œ ëª©ì°¨ êµ¬ì¡° : 4. ê¸°ìˆ ì‚¬í•­ > 4.3 ì°¨ëŸ‰íŠ¹ì„± > 4.3.8 ì œì–´ê³µê¸° ì••ë ¥  
ë‚´ìš©: "ì£¼ê³µê¸°(MR) 883kPa(9kgf/cmÂ²), ì œë™ì••ë ¥(BC) 490kPa ì´í•˜, ê°ì¢… ê³µì••ì œì–´ì¥ì¹˜ 490kPa(ë™ì‘ë²”ìœ„ 392ï½588kPa)"

ì‚¬ìš©ìê°€ ì°¾ëŠ” í•­ëª©ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì œê³µë©ë‹ˆë‹¤:
ì˜ˆì‹œ ì…ë ¥:  
- ë ˆë²¨1: ìš´ì˜ ì¡°ê±´  
- ë ˆë²¨2: ìš´ì˜ í™˜ê²½  
- ë ˆë²¨3: ìµœì € ì˜¨ë„  
- í‘œì¤€ë‹¨ìœ„: â„ƒ  

ë‹¹ì‹ ì˜ ì—­í• ì€ ë‹¤ìŒ ì¡°ê±´ì— ë§ëŠ” **5ê°œì˜ ê²€ìƒ‰ ì§ˆì˜(Query)**ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

1. ì‚¬ìš©ìê°€ ì°¾ê³ ì í•˜ëŠ” ì •ë³´ê°€ ë¬¸ì„œ ë‚´ í‘œí˜„ ë°©ì‹ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ **ì „ë¬¸ ìš©ì–´, ìœ ì˜ì–´, ë‹¤ì–‘í•œ ê¸°ìˆ ì  í‘œí˜„**ì„ í™œìš©í•´ì£¼ì„¸ìš”.
2. **ë¬¸ì¥í˜• ì¿¼ë¦¬ ë˜ëŠ” í•µì‹¬ì–´ ì¡°í•©í˜• ì¿¼ë¦¬**ê°€ í˜¼í•©ë˜ì–´ë„ ì¢‹ìŠµë‹ˆë‹¤ (ì˜ˆ: "ìµœì € ì˜¨ë„ëŠ”?" / "ìš´ì˜ ì¡°ê±´ ì˜¨ë„ â„ƒ").
3. **ìˆ«ì ë‹¨ìœ„(ì˜ˆ: mm, kPa, â„ƒ ë“±)ë„ ì¤‘ìš”í•œ ê²€ìƒ‰ í‚¤ì›Œë“œ**ê°€ ë  ìˆ˜ ìˆìœ¼ë‹ˆ í¬í•¨í•´ì£¼ì„¸ìš”.
4. ìµœì¢… ëª©ì ì€ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ê°€ì¥ ì˜ ì°¾ì„ ìˆ˜ ìˆëŠ” ì§ˆì˜ì–´ ì„¸íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ì¶œë ¥ ì˜ˆì‹œ:  
- "ì—´ì°¨ê°€ ìš´í–‰ ê°€ëŠ¥í•œ ìµœì € ì˜¨ë„ëŠ”?"  
- "ê¸°í›„ ì¡°ê±´ì—ì„œì˜ ìµœì € ì˜¨ë„ëŠ” ëª‡ â„ƒì¸ê°€ìš”?"  
- "ì—´ì°¨ì˜ ìµœì†Œ ìš´ì „ ê°€ëŠ¥ ì˜¨ë„"  
- "ìš´ì˜í™˜ê²½ ê¸°ì¤€ ìµœì € ê¸°ì˜¨ â„ƒ"  
- "train minimum operating temperature in Celsius"
    """
    level1 = (
        row.get("ë ˆë²¨1", "")
        if row.get("ë ˆë²¨1", "") and pd.notna(row.get("ë ˆë²¨1", ""))
        else ""
    )
    user_prompt = f"""ì´ì œ ì•„ë˜ í•­ëª©ì— ë”°ë¼ 5ê°œì˜ ê²€ìƒ‰ ì§ˆì˜ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
    ë ˆë²¨1: {level1}
    ë ˆë²¨2: {level2}
    ë ˆë²¨3: {level3} 
    ë ˆë²¨4: {level4}
    í‘œì¤€ë‹¨ìœ„: {std_unit}
    ì¿¼ë¦¬ëŠ” ìˆœì„œëŒ€ë¡œ ë‚˜ì—´ë§Œ í•´ì£¼ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë²ˆí˜¸ëŠ” í•„ìš” ì—†ìŠµë‹ˆë‹¤."""

    llm = ChatOpenAI(temperature=0.2, model="gpt-4o-mini")
    response = llm.predict(system_prompt + "\n\n" + user_prompt)
    # LLM ì‘ë‹µì„ ì¿¼ë¦¬ ëª©ë¡ìœ¼ë¡œ ë³€í™˜
    llm_queries = [q.strip() for q in response.strip().split("\n") if q.strip()]
    for i, q in enumerate(llm_queries):
        queries.append({"type": f"llm_generated_{i+1}", "query": q})
    print("queries::", queries)
    return queries


# ë¹ˆ ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™” í•¨ìˆ˜
def initialize_empty_dataframe():
    # CSVì—ì„œ í…Œì´ë¸” êµ¬ì¡° ë¡œë“œ
    table_df = load_table_data()
    result_df = table_df.copy()

    # ê²°ê³¼ ì—´ ì¶”ê°€ (ë¯¸ë¦¬ ì¶”ê°€í•˜ì—¬ ë°ì´í„° íƒ€ì… ë¬¸ì œ ë°©ì§€)
    if "LLMì‘ë‹µ" not in result_df.columns:
        result_df["LLMì‘ë‹µ"] = ""
    if "ì°¸ì¡°ë¬¸ì„œ" not in result_df.columns:
        result_df["ì°¸ì¡°ë¬¸ì„œ"] = None
    if "ì°¸ì¡°ë¬¸ì„œëª©ì°¨" not in result_df.columns:
        result_df["ì°¸ì¡°ë¬¸ì„œëª©ì°¨"] = ""
    if "ì •ë‹µì—¬ë¶€" not in result_df.columns:
        result_df["ì •ë‹µì—¬ë¶€"] = False
    if "ê²€ìƒ‰ì„±ê³µì—¬ë¶€" not in result_df.columns:
        result_df["ê²€ìƒ‰ì„±ê³µì—¬ë¶€"] = False

    return result_df


# ì„¸ì…˜ ìƒíƒœì— ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™” (ì•±ì´ ì²˜ìŒ ì‹¤í–‰ë  ë•Œë§Œ)
if "result_df" not in st.session_state:
    st.session_state.result_df = initialize_empty_dataframe()

# ì„¸ì…˜ ìƒíƒœì— ë²¡í„° DB ê²°ê³¼ ì´ˆê¸°í™”
if "vector_db_result" not in st.session_state:
    st.session_state.vector_db_result = None

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("ì„¤ì •")

    # íŒŒì¼ ì—…ë¡œë“œ ë²„íŠ¼
    uploaded_file = st.file_uploader("ì‚¬ì–‘ì„œ íŒŒì¼ ì—…ë¡œë“œ (DOCX)", type=["docx"])

    # LLM ëª¨ë¸ ì„ íƒ
    model_option = st.radio(
        "LLM ëª¨ë¸ ì„ íƒ", ["GPT-4", "Claude 3 Opus", "Claude 3 Sonnet"]
    )

    # ì¶”ì¶œ ì‹œì‘ ë²„íŠ¼
    start_button = st.button("ì‚¬ì–‘ ì¶”ì¶œ ì‹œì‘", type="primary")

    # ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
    reset_button = st.button("ê²°ê³¼ ì´ˆê¸°í™”")

    # ì¶”ê°€ ì •ë³´
    st.info(
        "ì´ ì•±ì€ RAG(Retrieval Augmented Generation)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì—ì„œ ì‚¬ì–‘ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
    )

# ë©”ì¸ ì½˜í…ì¸ 
if uploaded_file is not None:
    st.write("íŒŒì¼ì´ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤:", uploaded_file.name)

# ê²°ê³¼ ì´ˆê¸°í™” ì²˜ë¦¬
if reset_button:
    st.session_state.result_df = initialize_empty_dataframe()
    st.session_state.vector_db_result = None
    st.success("ê²°ê³¼ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ë°ì´í„° í”„ë ˆì„ ì˜ì—­ - í•­ìƒ í‘œì‹œ
st.subheader("ì—´ì°¨ ì‚¬ì–‘ ë¶„ì„ í•­ëª©")
result_placeholder = st.empty()
result_placeholder.dataframe(st.session_state.result_df, use_container_width=True)

# ì„±ëŠ¥ í‰ê°€ ë²„íŠ¼ ì˜ì—­
evaluate_placeholder = st.empty()

# ìƒíƒœ í‘œì‹œ ì˜ì—­
status_placeholder = st.empty()

# ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì˜ì—­
download_placeholder = st.empty()


# ì„±ëŠ¥ í‰ê°€ llm ì²˜ë¦¬
def evaluate_llm(pred, answer, answer_doc, ref_doc):
    llm = ChatOpenAI(temperature=0.2, model="gpt-4o-mini")
    ev_prompt = f"""ë‹¤ìŒì€ ì •ë‹µê³¼ ì •ë‹µì˜ ê·¼ê±° ë¬¸ì„œ ì…ë‹ˆë‹¤.
    ì •ë‹µ: {pred}
    ì •ë‹µ ì¶œì²˜: {answer_doc}
    
    ìœ„ ì •ë‹µê³¼ ìë™ ì¶”ì¶œ ê²°ê³¼ ê°’ì´ ê°™ì€ì§€ true/falseë¡œ ë‹µí•´ì£¼ì„¸ìš”. ìˆ«ì ê°’ì´ë‚˜ ë‹¨ìœ„, ì„¤ëª… ë“± ê°™ì€ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ê²½ìš° ê°™ì€ ê°’ìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
    ë˜í•œ ragë¡œ ê²€ìƒ‰í•œ ì²­í¬ë“¤ ì¤‘ ìœ„ ì •ë‹µ ì¶œì²˜ ë¬¸ì„œ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ true/falseë¡œ ë‹µí•´ì£¼ì„¸ìš”. 
    ì¶”ì¶œ ê²°ê³¼: {answer}
    rag ê²€ìƒ‰í•œ ì°¸ì¡° ì²­í¬ë“¤: {ref_doc}

    ë‹µë³€ í˜•ì‹:
    ì •ë‹µ ì¼ì¹˜ ì—¬ë¶€: true/false
    ì²­í¬ ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€: true/false
    """
    response = llm.predict(ev_prompt)
    print("evaluate_llm::", response)

    answer_correct = (
        response.split("ì •ë‹µ ì¼ì¹˜ ì—¬ë¶€:")[1].split("ì²­í¬ ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€:")[0].strip()
        == "true"
    )
    search_success = response.split("ì²­í¬ ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€:")[1].strip() == "true"

    return answer_correct, search_success


# ì„±ëŠ¥ í‰ê°€ ì²˜ë¦¬
def evaluate_performance():
    # ì •ë‹µì—¬ë¶€ì™€ ê²€ìƒ‰ì„±ê³µì—¬ë¶€ í•œë²ˆì— ê³„ì‚°
    results = st.session_state.result_df.apply(
        lambda row: evaluate_llm(
            row["LLMì‘ë‹µ"], row["ì •ë‹µ"], row["ì •ë‹µ ë¬¸ì„œ"], row["ì°¸ì¡°ë¬¸ì„œ"]
        ),
        axis=1,
    )

    # ê²°ê³¼ë¥¼ ê° ì—´ì— í• ë‹¹
    st.session_state.result_df["ì •ë‹µì—¬ë¶€"] = [result[0] for result in results]
    st.session_state.result_df["ê²€ìƒ‰ì„±ê³µì—¬ë¶€"] = [result[1] for result in results]

    # ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼ë§ í•¨ìˆ˜
    def highlight_correct(row):
        if row["ì •ë‹µì—¬ë¶€"]:
            return [
                "background-color: #CCFFCC" if col == "LLMì‘ë‹µ" else ""
                for col in row.index
            ]
        else:
            return [
                "background-color: #FFCCCC" if col == "LLMì‘ë‹µ" else ""
                for col in row.index
            ]

    # ìŠ¤íƒ€ì¼ë§ ì ìš©
    styled_df = st.session_state.result_df.style.apply(highlight_correct, axis=1)

    # ê²°ê³¼ í‘œì‹œ
    result_placeholder.dataframe(styled_df, use_container_width=True)

    # ì •ë‹µë¥  ê³„ì‚°
    correct = st.session_state.result_df["ì •ë‹µì—¬ë¶€"].sum()
    total = len(st.session_state.result_df)
    accuracy = correct / total if total > 0 else 0

    # ê²€ìƒ‰ ì¬í˜„ìœ¨ ê³„ì‚°
    hit = st.session_state.result_df["ê²€ìƒ‰ì„±ê³µì—¬ë¶€"].sum()
    recall = hit / total if total > 0 else 0

    st.session_state.evaluation_result = (
        f"ì •ë‹µë¥ : {accuracy:.0%} ({correct}/{total}), "
        f"ê²€ìƒ‰ ì¬í˜„ìœ¨: {recall:.0%} ({hit}/{total})"
    )


# ë¶„ì„ ì²˜ë¦¬
if start_button and uploaded_file is not None:
    # ìƒíƒœ ë©”ì‹œì§€ í‘œì‹œ
    status_placeholder.info("ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ë²¡í„° DB ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    try:
        chunks = convert_docx_to_chunks(uploaded_file)

        # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = OpenAIEmbeddings()
        print("embeddings ë³€ìˆ˜ ì´ˆê¸°í™” ì™„ë£Œ")

        # Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„± - ê³µì‹ ë¬¸ì„œ ë°©ì‹ëŒ€ë¡œ
        vectorstore = Chroma(
            collection_name="langchain",
            embedding_function=embeddings,
            # persist_directory=persist_directory,
        )
        k = 5
        vectorstore.add_documents(chunks)
        vector_retriever = vectorstore.as_retriever(search_kwargs={"k": k})

        # BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        bm25_retriever = BM25Retriever.from_documents(chunks)

        ensemble = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[0.5, 0.5],
        )

        # ë¬¸ì„œ ì²­í‚¹/ë²¡í„°í™” ì™„ë£Œ í›„ ë°ì´í„° ì¶”ì¶œ ì‹œì‘
        status_placeholder.success("ë¬¸ì„œ ë²¡í„°í™” ì™„ë£Œ. ì‚¬ì–‘ ì •ë³´ ì¶”ì¶œ ì¤‘...")

        total_rows = len(st.session_state.result_df)
        progress_bar = st.progress(0)

        for index, row in st.session_state.result_df.iterrows():
            # ì§„í–‰ ìƒíƒœ í‘œì‹œ
            progress = (index + 1) / total_rows
            progress_bar.progress(progress)

            status_placeholder.info(f"í•­ëª© {index+1}/{total_rows} ì²˜ë¦¬ ì¤‘...")

            ensemble_docs = []
            queries = generate_queries(row)
            for query in queries:
                docs = ensemble.get_relevant_documents(query["query"])
                print("docs::", docs)
                ensemble_docs.extend(docs)
            # ì¤‘ë³µ ì œê±°
            unique_docs = {}
            for doc in ensemble_docs:
                # doc ê°ì²´ì˜ id í•„ë“œë¥¼ ì‚¬ìš©
                if hasattr(doc, "id"):
                    doc_key = doc.id
                else:
                    doc_key = doc.page_content
                unique_docs[doc_key] = doc
            ensemble_docs = list(unique_docs.values())
            # ë¹ˆ ì¿¼ë¦¬ ì œì™¸
            filtered_queries = [
                query["query"] for query in queries if query["query"].strip()
            ]
            query_used = ", ".join(filtered_queries)
            answer_prompt = f"""ë‹¤ìŒì€ ì—´ì°¨ ì‚¬ì–‘ì„œì˜ ì—¬ëŸ¬ ë¶€ë¶„ì…ë‹ˆë‹¤:
{ensemble_docs}
ìœ„ ë¬¸ì„œì—ì„œ ì•„ë˜ ì§ˆì˜ë“¤ì— ëŒ€í•œ ë‹µë³€ê³¼ ì°¸ì¡° ë¬¸ì„œì˜ ëª©ì°¨ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.  
1), ê°€) ë“± ì„¸ë¶€ ëª©ì°¨ êµ¬ë¶„ìëŠ” í¬í•¨í•˜ë˜, **ë¬¸ì„œ ì œëª©ì´ë‚˜ ë‚´ìš©ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.** ì•„ë˜ ì§ˆì˜ë“¤ì€ ëª¨ë‘ ë™ì¼ í•­ëª©ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.  
ë¬¸ì„œì— ê°’ì´ ëª…ì‹œë˜ì–´ ìˆë‹¤ë©´ ë‹¨ìœ„ë„ í•¨ê»˜ í‘œì‹œí•´ì£¼ì„¸ìš”.  

ğŸ’¡ **ë°˜ë“œì‹œ ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ ê°„ê²°í•œ í˜•ì‹**ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:  
ì˜ˆì‹œ1)  
ë‹µë³€: 1000mm, ì°¸ì¡°ë¬¸ì„œ: 3.3 1)  
ì˜ˆì‹œ2)  
ë‹µë³€: ì² ë„ ê·œì •ì— ë”°ë¥¸ë‹¤., ì°¸ì¡°ë¬¸ì„œ: 4.3.8 1)  
ì˜ˆì‹œ3)  
ë‹µë³€: 75dB, ì°¸ì¡°ë¬¸ì„œ: 4.3.11.2 1) ê°€)

â—ì°¸ì¡°ë¬¸ì„œëŠ” **ìˆ«ì, 1), ê°€)** ë“± **ëª©ì°¨ ì •ë³´ë§Œ í¬í•¨í•˜ê³  ì œëª©ì´ë‚˜ ë³¸ë¬¸ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**  
ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì§ˆë¬¸: '{query_used}'"""

            llm = ChatOpenAI(temperature=0.2, model="gpt-4o-mini")
            answer = llm.predict(answer_prompt)
            values = answer.split("ë‹µë³€:")[1].split(", ì°¸ì¡°ë¬¸ì„œ:")[0].strip()
            doc_index = answer.split("ì°¸ì¡°ë¬¸ì„œ:")[-1].strip()
            print("ë‹µë³€::", values)
            print("ì°¸ì¡°ë¬¸ì„œ::", doc_index)

            # ê°’ í• ë‹¹ ì‹œ ë°ì´í„° íƒ€ì… ë¬¸ì œ ë°©ì§€
            st.session_state.result_df.loc[index, "LLMì‘ë‹µ"] = str(values)
            st.session_state.result_df.loc[index, "ì°¸ì¡°ë¬¸ì„œëª©ì°¨"] = str(doc_index)

            # ê°ì²´ë¥¼ ì§ì ‘ ì €ì¥í•˜ë©´ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
            st.session_state.result_df.loc[index, "ì°¸ì¡°ë¬¸ì„œ"] = str(ensemble_docs)

            # ê° í•­ëª© ì²˜ë¦¬ í›„ UI ì—…ë°ì´íŠ¸
            result_placeholder.dataframe(
                st.session_state.result_df, use_container_width=True
            )

        progress_bar.empty()
        evaluate_performance()
        if st.session_state.evaluation_result:
            status_placeholder.success(st.session_state.evaluation_result)

        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í™œì„±í™”
        csv = st.session_state.result_df.to_csv(index=False)
        download_placeholder.download_button(
            label="CSVë¡œ ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name="ì—´ì°¨_ì‚¬ì–‘_ì¶”ì¶œê²°ê³¼.csv",
            mime="text/csv",
        )

    except Exception as e:
        status_placeholder.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

elif start_button and uploaded_file is None:
    st.error("ë¨¼ì € ì‚¬ì–‘ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
